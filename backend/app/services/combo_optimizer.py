"""
Combo Strategy Optimizer

Adapts SequentialOptimizer for combo strategies with multiple indicators.
Handles parameter naming, stage generation, and optimization execution.
"""

import json
import concurrent.futures
import os
import time
import logging
from typing import Dict, List, Any, Optional
from pathlib import Path

from app.services.sequential_optimizer import SequentialOptimizer
from app.services.combo_service import ComboService
from app.services.backtest_service import BacktestService
from src.data.incremental_loader import IncrementalLoader



def _worker_run_backtest(args):
    """
    Worker function to run a single backtest in a separate process.
    Must be top-level to be picklable.
    """
    template_data, params, df, stage_param, value = args
    
    try:
        # Reconstruct strategy logic locally to avoid DB connection in worker
        indicators = template_data["indicators"]
        entry_logic = template_data["entry_logic"]
        exit_logic = template_data["exit_logic"]
        stop_loss = template_data.get("stop_loss", 0.015)
        
        # Handle stop_loss if it's a dict with 'default' key
        if isinstance(stop_loss, dict):
            stop_loss = stop_loss.get("default", 0.015)
            
        # Apply parameter overrides
        # Deep copy indicators to avoid modifying shared state (though processes duplicate memory anyway)
        import copy
        indicators = copy.deepcopy(indicators)
        
        if params:
            for param_key, param_value in params.items():
                if param_key == "stop_loss":
                    stop_loss = param_value
                    continue
                
                # Check for timeframe (handled outside, but ignore if present)
                if param_key == "timeframe":
                    continue

                # Find matching indicator for this parameter
                matched = False
                for indicator in indicators:
                    alias = indicator.get("alias", "")
                    type_ = indicator.get("type", "")
                    
                    # 1. Try "alias_param" format (e.g., "short_length") - Generated by auto-schema
                    if alias and param_key.startswith(f"{alias}_"):
                        target_field = param_key[len(alias)+1:]
                        if "params" not in indicator: indicator["params"] = {}
                        indicator["params"][target_field] = param_value
                        matched = True
                        break
                    
                    # 2. Try "type_alias" format (e.g., "sma_short") - Used in multi_ma_crossover
                    # Default to 'length' or 'period' if not specified
                    if alias and type_ and param_key == f"{type_}_{alias}":
                        if "params" not in indicator: indicator["params"] = {}
                        # Try to find which param to update: length, period, or default to length
                        if "length" in indicator["params"]:
                            indicator["params"]["length"] = param_value
                        elif "period" in indicator["params"]:
                            indicator["params"]["period"] = param_value
                        else:
                            indicator["params"]["length"] = param_value # Fallback
                        matched = True
                        break

                    # 3. Try exact alias match (e.g. "short")
                    if alias and param_key == alias:
                        if "params" not in indicator: indicator["params"] = {}
                        if "length" in indicator["params"]:
                            indicator["params"]["length"] = param_value
                        elif "period" in indicator["params"]:
                            indicator["params"]["period"] = param_value
                        else:
                            indicator["params"]["length"] = param_value
                        matched = True
                        break

                if not matched:
                    # Fallback: simple containment if unique?
                    # or just log warning (cannot log easily in worker)
                    pass

        # Create strategy instance
        from app.strategies.combos import ComboStrategy
        strategy = ComboStrategy(
            indicators=indicators,
            entry_logic=entry_logic,
            exit_logic=exit_logic,
            stop_loss=stop_loss
        )
        
        # Generate signals
        df_with_signals = strategy.generate_signals(df.copy())
        
        # Extract trades from signals
        trades = []
        position = None
        
        for idx, row in df_with_signals.iterrows():
            if row['signal'] == 1 and position is None:
                # Buy signal
                position = {
                    'entry_time': idx,
                    'entry_price': row['close'],
                    'type': 'long'
                }
            elif row['signal'] == -1 and position is not None:
                # Sell signal
                position['exit_time'] = idx
                position['exit_price'] = row['close']
                position['profit'] = (position['exit_price'] - position['entry_price']) / position['entry_price']
                trades.append(position)
                position = None
        
        # Calculate metrics
        metrics = {
            'total_trades': 0,
            'win_rate': 0,
            'total_return': 0,
            'avg_profit': 0,
            'sharpe_ratio': 0
        }

        if len(trades) > 0:
            total_trades = len(trades)
            winning_trades = sum(1 for t in trades if t['profit'] > 0)
            metrics['total_trades'] = total_trades
            metrics['win_rate'] = winning_trades / total_trades
            metrics['total_return'] = sum(t['profit'] for t in trades)
            metrics['avg_profit'] = metrics['total_return'] / total_trades
            
            # Simple Sharpe approximation
            returns = [t['profit'] for t in trades]
            import numpy as np
            std_dev = np.std(returns)
            metrics['sharpe_ratio'] = np.mean(returns) / std_dev if std_dev > 0 else 0
        
        return {
            'value': value,
            'metrics': metrics,
            'trades_count': len(trades),
            'success': True
        }
        
    except Exception as e:
        return {
            'value': value,
            'error': str(e),
            'success': False
        }


class ComboOptimizer:
    """
    Optimizer for combo strategies.
    
    Extends SequentialOptimizer logic to handle multiple indicators
    and their parameters in combo strategies.
    """
    
    def __init__(self, checkpoint_dir: str = "backend/data/checkpoints"):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.combo_service = ComboService()
        self.backtest_service = BacktestService()
        self.loader = IncrementalLoader()
    
    def generate_stages(
        self,
        template_name: str,
        symbol: str,
        fixed_timeframe: Optional[str] = None,
        custom_ranges: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Generate optimization stages for a combo strategy.
        
        Args:
            template_name: Name of combo template
            symbol: Trading symbol
            fixed_timeframe: If set, skips timeframe optimization
            custom_ranges: Custom parameter ranges
            
        Returns:
            List of stage configurations
        """
        # Get template metadata
        metadata = self.combo_service.get_template_metadata(template_name)
        
        stages = []
        stage_num = 1
        
        # Stage 1: Timeframe optimization (if not fixed)
        if not fixed_timeframe:
            stages.append({
                "stage_num": stage_num,
                "stage_name": "Timeframe",
                "parameter": "timeframe",
                "values": ["1m", "5m", "15m", "30m", "1h", "4h", "1d"],
                "locked_params": {},
                "description": "Optimize trading timeframe"
            })
            stage_num += 1
            
        # Get optimization schema from database (if available)
        schema = metadata.get('optimization_schema')
                
        if schema:
            # Use explicit schema from database
            for param_name, config in schema.items():
                # Check for custom range override
                if custom_ranges and param_name in custom_ranges:
                    custom = custom_ranges[param_name]
                    values = self._generate_values_from_range(
                        custom.get('min', config['min']),
                        custom.get('max', config['max']),
                        custom.get('step', config['step'])
                    )
                else:
                    values = self._generate_values_from_range(
                        config['min'],
                        config['max'],
                        config['step']
                    )
                
                stages.append({
                    "stage_num": stage_num,
                    "stage_name": f"Optimize {param_name}",
                    "parameter": param_name,
                    "values": values,
                    "locked_params": {},
                    "description": f"Optimize {param_name}"
                })
                stage_num += 1
                
        else:
            # Fallback: Infer from indicators metadata (Legacy behavior for templates without schema)
            # Stages 2 to N: Indicator parameters
            for indicator in metadata['indicators']:
                ind_type = indicator['type']
                ind_alias = indicator.get('alias', ind_type)
                ind_params = indicator.get('params', {})
                
                # Get optimization ranges from indicator config or use defaults
                opt_range = indicator.get('optimization_range', {})
                
                for param_name, param_value in ind_params.items():
                    # Create full parameter name (e.g., "ema_fast_length")
                    full_param_name = f"{ind_alias}_{param_name}"
                    
                    # Check if custom range is provided
                    if custom_ranges and full_param_name in custom_ranges:
                        custom = custom_ranges[full_param_name]
                        values = self._generate_values_from_range(
                            custom.get('min', param_value),
                            custom.get('max', param_value * 2),
                            custom.get('step', 1)
                        )
                    elif param_name in opt_range:
                        # Use optimization range from template
                        range_config = opt_range[param_name]
                        values = self._generate_values_from_range(
                            range_config.get('min', param_value),
                            range_config.get('max', param_value * 2),
                            range_config.get('step', 1)
                        )
                    else:
                        # Default range: Â±50% of default value
                        min_val = max(1, int(param_value * 0.5))
                        max_val = int(param_value * 1.5)
                        step = max(1, int((max_val - min_val) / 10))
                        values = self._generate_values_from_range(min_val, max_val, step)
                    
                    stages.append({
                        "stage_num": stage_num,
                        "stage_name": f"{ind_alias.upper()} {param_name}",
                        "parameter": full_param_name,
                        "values": values,
                        "locked_params": {},
                        "description": f"Optimize {ind_alias} {param_name}"
                    })
                    stage_num += 1
        
        return stages
    
    def _generate_values_from_range(self, min_val: float, max_val: float, step: float) -> List[float]:
        """Generate list of values from min to max with step."""
        values = []
        current = min_val
        while current <= max_val:
            values.append(current)
            current += step
        return values
    


    def run_optimization(
        self,
        template_name: str,
        symbol: str,
        timeframe: str = "1h",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        custom_ranges: Optional[Dict[str, Any]] = None,
        job_id: Optional[str] = None
    ) -> Dict[str, Any]:
        
        # Generate stages
        fixed_timeframe = timeframe if timeframe else None
        stages = self.generate_stages(
            template_name=template_name,
            symbol=symbol,
            fixed_timeframe=fixed_timeframe,
            custom_ranges=custom_ranges
        )
        
        # Get template metadata ONCE for workers
        template_metadata = self.combo_service.get_template_metadata(template_name)
        
        # Load data
        df = self.loader.fetch_data(
            symbol=symbol,
            timeframe=timeframe,
            since_str=start_date,
            until_str=end_date
        )
        
        # Initialize best parameters
        best_params = {}
        best_metrics = None
        
        # Use ProcessPoolExecutor for parallel execution
        # Use max_workers = CPU count - 1 to leave one for the OS/Backend
        max_workers = max(1, (os.cpu_count() or 2) - 1)
        
        # Run optimization stages
        for stage in stages:
            stage_param = stage['parameter']
            stage_values = stage['values']
            
            logging.info(f"Stage {stage['stage_num']}: Optimizing {stage_param} ({len(stage_values)} tests) with {max_workers} workers")
            
            start_time = time.time()
            stage_best_value = None
            stage_best_sharpe = float('-inf')
            
            # Prepare arguments for parallel execution
            worker_args = []
            for value in stage_values:
                # Build parameters for this test
                test_params = best_params.copy()
                
                # Handle timeframe parameter logic in main process if possible, 
                # but since we pass DF, we can't easily change timeframe in worker for same DF.
                # NOTE: Parallelizing timeframe optimization is tricky because DF changes.
                # For now, let's keep timeframe optimization sequential or assume fixed DF for other params.
                
                if stage_param == 'timeframe':
                    # Timeframe optimization changes the DATA itself. 
                    # Complex to parallelize efficiently without reloading data in every worker.
                    # Fallback to sequential for timeframe stage
                    continue
                else:
                    test_params[stage_param] = value
                    worker_args.append((template_metadata, test_params, df, stage_param, value))
            
            if stage_param == 'timeframe':
                # Sequential fallback for timeframe
                for value in stage_values:
                    # ... (existing sequential logic for timeframe) ...
                     # Reload data with new timeframe
                    sub_df = self.loader.fetch_data(
                        symbol=symbol,
                        timeframe=value,
                        since_str=start_date,
                        until_str=end_date
                    )
                     # Run single test (simplified sequential)
                    logging.info(f"  Testing {stage_param}={value}")
                    # ... execute ...
                    # Should replicate worker logic but sequentially
                    # For brevity, reusing worker function logic here would be clean but dataframe differs.
                    # Synchronous execution for timeframe
                    worker_args = (template_metadata, test_params, sub_df, stage_param, value)
                    res = _worker_run_backtest(worker_args)
                    
                    value = res['value']
                    if res['success']:
                        metrics = res['metrics']
                        trades_count = res['trades_count']
                        sharpe = metrics['sharpe_ratio']
                        
                        logging.info(f"  Tested {stage_param}={value} -> Sharpe: {sharpe:.3f}, Trades: {trades_count}")
                        
                        if sharpe > stage_best_sharpe:
                            stage_best_sharpe = sharpe
                            stage_best_value = value
                            best_metrics = metrics
                    else:
                        logging.error(f"  Failed {stage_param}={value}: {res.get('error')}")
            
            else:
                # Parallel execution for indicator parameters
                with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
                    results = list(executor.map(_worker_run_backtest, worker_args))
                
                # Process results with Weighted Composite Score
                # Collect all valid results first
                valid_results = []
                for res in results:
                    if res['success']:
                        valid_results.append(res)
                    else:
                        logging.error(f"  Failed {stage_param}={res['value']}: {res.get('error')}")

                if valid_results:
                    # Calculate min/max for normalization
                    sharpes = [r['metrics']['sharpe_ratio'] for r in valid_results]
                    returns = [r['metrics']['total_return'] for r in valid_results]
                    
                    min_sharpe, max_sharpe = min(sharpes), max(sharpes)
                    min_return, max_return = min(returns), max(returns)
                    
                    range_sharpe = max_sharpe - min_sharpe
                    range_return = max_return - min_return
                    
                    best_score = float('-inf')

                    for res in valid_results:
                        metrics = res['metrics']
                        sharpe = metrics['sharpe_ratio']
                        total_return = metrics['total_return']
                        
                        # Normalize inputs (0 to 1 scale)
                        # Handle division by zero if all values are identical
                        norm_sharpe = (sharpe - min_sharpe) / range_sharpe if range_sharpe > 0 else 0
                        norm_return = (total_return - min_return) / range_return if range_return > 0 else 0
                        
                        # Weighted Score: 70% Sharpe, 30% Total Return
                        # If values are identical, score will be 0, which is fine
                        score = (0.7 * norm_sharpe) + (0.3 * norm_return)
                        
                        # Tie-breaker: If scores exact, prefer higher Sharpe
                        if score > best_score:
                            best_score = score
                            stage_best_value = res['value']
                            best_metrics = metrics
                            stage_best_sharpe = sharpe # Still track for logging

                        logging.info(f"  Tested {stage_param}={res['value']} -> Sharpe: {sharpe:.3f}, Return: {total_return:.3f} (Score: {score:.3f})")

            # Update best params for next stage
            if stage_best_value is not None:
                best_params[stage_param] = stage_best_value
                logging.info(f"Stage {stage['stage_num']} complete: {stage_param}={stage_best_value} (Sharpe: {stage_best_sharpe:.3f})")
            else:
                logging.warning(f"Stage {stage['stage_num']} failed to find improvements. Keeping default/first.")
                if stage_values:
                    best_params[stage_param] = stage_values[0]

            logging.info(f"Stage time: {time.time() - start_time:.2f}s")


        # Run final backtest with best parameters to get complete data
        logging.info(f"Running final backtest with best parameters: {best_params}")
        
        try:
            # Reload data with best timeframe
            df_final = self.loader.fetch_data(
                symbol=symbol,
                timeframe=timeframe,
                since_str=start_date,
                until_str=end_date
            )
            
            # Create strategy with best parameters
            strategy = self.combo_service.create_strategy(
                template_name=template_name,
                parameters=best_params
            )
            
            # Generate signals
            df_with_signals = strategy.generate_signals(df_final.copy())
            
            # Extract trades
            trades = []
            position = None
            
            for idx, row in df_with_signals.iterrows():
                if row['signal'] == 1 and position is None:
                    position = {
                        'entry_time': str(idx),
                        'entry_price': float(row['close']),
                        'type': 'long'
                    }
                elif row['signal'] == -1 and position is not None:
                    position['exit_time'] = str(idx)
                    position['exit_price'] = float(row['close'])
                    position['profit'] = (position['exit_price'] - position['entry_price']) / position['entry_price']
                    trades.append(position)
                    position = None
            
            # Prepare candles data
            candles = []
            for idx, row in df_final.iterrows():
                candles.append({
                    'timestamp_utc': str(idx),
                    'open': float(row['open']),
                    'high': float(row['high']),
                    'low': float(row['low']),
                    'close': float(row['close']),
                    'volume': float(row['volume'])
                })
            
            # Extract indicator data
            indicator_data = {}
            for col in df_with_signals.columns:
                if col not in ['open', 'high', 'low', 'close', 'volume', 'signal']:
                    indicator_data[col] = df_with_signals[col].fillna(0).tolist()
            
        except Exception as e:
            logging.error(f"Final backtest failed: {e}")
            trades = []
            candles = []
            indicator_data = {}
        
        return {
            "job_id": job_id or "combo_opt_" + str(hash(template_name + symbol)),
            "template_name": template_name,
            "symbol": symbol,
            "timeframe": timeframe,
            "stages": stages,
            "best_parameters": best_params,
            "best_metrics": best_metrics or {},
            "total_stages": len(stages),
            # Add complete backtest data
            "trades": trades,
            "candles": candles,
            "indicator_data": indicator_data,
            "parameters": best_params  # For compatibility with ComboResultsPage
        }
